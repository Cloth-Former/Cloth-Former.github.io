<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>iQIYI Video Virtual Try-on (iQIYI-VVT) dataset</title>

  <style type="text/css">
    body,
    td,
    th {
      font-size: 16px;
    }

    body {
      border: 2px rgba(255, 255, 255, .38) solid;
      border-radius: 4px;
      margin-left: 300px;
      margin-right: 300px;
      margin-top: 18px;
    }

    pre {
      font-size: 12pt;
      background-color: #fffff4;
      border: 1px solid #999;
      line-height: 20px;
    }
  </style>

</head>
<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          <font size="7">
            <center>iQIYI Video Virtual Try-on (iQIYI-VVT) dataset</center>
          </font>
        </h1>
      </div>

      <div class="section details">
        <a id="dataset"></a>
        <h2>
          <font face="Tahoma">Details</font><br>
        </h2>
        <p style="text-align:justify"><b>iQIYI Video Virtual Try-on (iQIYI-VVT) dataset</b> is a wild virtual try-on dataset in video format.
          All model videos and in-shop garment images are downloaded from the Internet. To enhance temporal consistency,
          each video in iQIYI-VVT is divided into several coherent sub videos by adopting shot transition detection.
          Different from most virtual try-on datasets, model videos in iQIYI-VVT are of great variability in model's poses, environments,
          and occlusions. Clothes images in this dataset also include a variety of styles and colors. There are total <b>3994</b>
          video clips, <b>205100</b> frames in iQIYI-VVT. In addition to providing the videos in their original resolutions, we 
          also provide the videos in 256x192 size and supporting information for scholars can use directly, including:

        </p>

        <ul>
          <li>
            <p>model videos(the person was cropped and padding, then resized to 256x192 resolution),</p>
          </li>
          <li>
            <p><a href="https://github.com/facebookresearch/detectron2">DensePose</a> labels,</p>
          </li>
          <li>
            <p>parsing results extracted by <a href="https://github.com/Engineering-Course/CIHP_PGN">CIHP_GPN</a>,
            </p>
          </li>
          <li>
            <p>matting results extracted by <a href="https://github.com/ZHKKKe/MODNet">MODNet</a>,
            </p>
          </li>
          <li>
            <p>body keypoints detected by <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">openpose</a>.</p>
          </li>
        </ul>
        <p style="text-align:justify">
        To be able to objectively measure the performance for
        the followers' entries, the dataset has been split into a training set and a test set where the size of
        training set is nine times larger than test set. In order to protect the provacy of models, we have blurred their faces in videos.
        </p>
      </div><br>

      <div class="section overview">
        <h2>
          <font face="Tahoma">Samples</font>
        </h2>
        <br>
        <!-- <center><img src="./Real-world Affective Faces (RAF) dataset_files/RAF-DB.png" border="0" width="80%">
        </center> -->
        <div class="content">
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/000041_1_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="27%">
            <source src="./dataset_samples/000158_8_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/000164_2_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/000434_2_demo.mp4" type="video/mp4">
          </video>
          <br>
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/000603_1_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="27%">
            <source src="./dataset_samples/001577_5_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/000993_20_demo.mp4" type="video/mp4">
          </video>
          <video autoplay loop muted playinline width="23%">
            <source src="./dataset_samples/002007_10_demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <br>




      <div class="section contact">
        <h2>
          <font face="Tahoma">Terms &amp; Conditions</font>
        </h2>
        <ul>
          <li>
            <p style="text-align:justify">The iQIYI-VVT dataset is available for <b>non-commercial research purposes</b> only.</p>
          </li>
          <li>
            <p style="text-align:justify">All model videos and clothes images of the iQIYI-VVT dataset are obtained from the internet which are not
              property of iQIYI Inc. The iQIYI Inc is not responsible for the content nor the meaning of
              these images.</p>
          </li>
          <li>
            <p style="text-align:justify">You agree <b>not to</b> reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial
              purposes, any portion of the images and any portion of derived data.</p>
          </li>
          <li>
            <p style="text-align:justify">You agree <b>not to</b> further copy, publish or distribute any portion of the iQIYI-VVT dataset. Exceptly, for
              internal use at a single site within the same organization it is allowed to make copies of the dataset.
            </p>
          </li>
          <li>
            <p style="text-align:justify">The iQIYI Inc reserves the right to terminate your access to the iQIYI-VVT dataset at any time.</p>
          </li>
        </ul>
      </div><br>

      <div class="password">
        <h2>
          <font face="Tahoma"> How to get iQIYI-VVT</font>
        </h2><br>
        <p style="text-align:justify">
        This dataset is publicly available. It is free for professors and researcher scientists affiliated to Academic Institutions.
        Permission to use but not reproduce or distribute the iQIYI-VVT dataset is granted to all researchers given that the
        following steps are properly followed:
        </p>
        <p style="text-align:justify">Send an e-mail to <a href="mailto:yanhe@qiyi.com">He Yan</a> or <a href="mailto:wangtan@qiyi.com">Tan Wang</a> to apply the dataset.
          You will receive a download link and password to access the files of this dataset. Your Email MUST include the following text:</p>



        <pre>
<b>Subject</b>: Application to download iQIYI-VVT dataset          
<b>Name</b>: &lt;your first and last name&gt;
<b>Affiliation</b>: &lt;Academic institution where you work&gt;
<b>Department</b>: &lt;your department&gt;
<b>Position</b>: &lt;your job title&gt;
<b>Email</b>: &lt;must be the email at the above mentioned institution&gt;<br>
I have read and agree to the terms and conditions specified in the iQIYI-VVT webpage. 
This dataset will only be used for research purposes. 
I will not make any part of this dataset available to a third party. 
I'll not sell any part of this dataset or make any profit from its use.
      </pre>


      </div><br>

      <div class="section download">
        <h2>
          <font face="Tahoma">Content Preview</font>
        </h2>
        <pre><code>     -<span class="ruby"> train&amp;test set
        </span>  -<span class="ruby"> {vid}
        </span>      -<span class="ruby"> {vid}_imgs ------------------ model video
        </span>          -<span class="ruby"> <span class="hljs-number">00000001</span>.png
        </span>          -<span class="ruby"> <span class="hljs-number">00000002</span>.png
        </span>          ...
              -<span class="ruby"> {vid}_densepose ------------- densepose labels
        </span>          -<span class="ruby"> <span class="hljs-number">00000001</span>.npy
        </span>          -<span class="ruby"> <span class="hljs-number">00000002</span>.npy
        </span>          ...
              -<span class="ruby"> {vid}_landmark -------------- body keypoints
        </span>          -<span class="ruby"> <span class="hljs-number">00000001</span>.json
        </span>          -<span class="ruby"> <span class="hljs-number">00000002</span>.json
        </span>          ...
              -<span class="ruby"> {vid}_matte ----------------- matting results
        </span>          -<span class="ruby"> <span class="hljs-number">00000001</span>.png
        </span>          -<span class="ruby"> <span class="hljs-number">00000002</span>.png
        </span>          ...
              -<span class="ruby"> {vid}_parse ----------------- parsing results
        </span>          -<span class="ruby"> <span class="hljs-number">00000001</span>.png
        </span>          -<span class="ruby"> <span class="hljs-number">00000002</span>.png
        </span>      -<span class="ruby"> {vid}_color.png ------------- garment's image
        </span>      -<span class="ruby"> {vid}_mask.png -------------- garment's mask
        </span>  ...
        
     -<span class="ruby"> raw_videos
        </span>  -<span class="ruby"> {vid}.mp4 ----------------------- model video
        </span>  -<span class="ruby"> {vid}_color.png ----------------- garment's image
        </span>  -<span class="ruby"> {vid}_mask.png ------------------ garment's mask
        </span>  ...
        </code></pre>
      </div>


      <div class="section citation">
        <h2>
          <font face="Tahoma">Citation</font>
        </h2>
        <div class="section bibtex">
          <pre>@inproceedings{jiang2022clothformer,
    title={ClothFormer: Taming Video Virtual Try-on in All Module},
    author={Jianbin Jiang and Tan Wang and He Yan and Junhui Liu},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}</pre>

      </div>
      </div>
      
      <div class="section contact">
        <h2>
          <font face="Tahoma">Contact</font>
        </h2>

        Please contact <a href="mailto:yanhe@qiyi.com">He Yan</a> and <a href="mailto:wangtan@qiyi.com">Tan Wang
        </a> for questions about the dataset.
      </div><br>
    </div>
  </div>
</body>

</html>